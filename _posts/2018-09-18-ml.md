---
title: CS-443 Machine Learning
description: "My notes from the CS-443 Machine Learning course given at EPFL, in the 2018 autumn semester (MA1)"
unlisted: true
---

* TOC
{:toc}

We'll always use subscript $$n$$ for data point, and $$d$$ for feature. $$N$$ is the data size and $$D$$ is the dimensionality.

## Linear regression
A linear regression is a model that assumes a linear relationship between inputs and the output.

### Simple linear regression

For a single input dimension ($$D=1$$), we can use a simple linear regression, which is given by:
$$
y_n \approx f(x_n) := w_0 + w_1 x_{n1}
$$

$$w = (w_0, w_1)$$ are the parameters of the model.

### Multiple linear regression

If our data has multiple input dimensions, we obtain multivariate linear regression:

$$
y_n \approx 
    f(\pmb{x}_n) :=w_0 + w_1x_{n1}+\dots+w_D x_{wD} 
    = w_0 + \pmb{x}_n^t \begin{bmatrix}
        w_1 \\
        \vdots \\
        w_D \\
    \end{bmatrix}
    = \tilde{\pmb{x}}_n ^T \tilde{\pmb{w}}
$$

> ðŸ‘‰ðŸ¼ If we wanted to be a little more strict, we should write $$f_{\pmb{w}}(\pmb{x}_n)$$, as the model of course also depends on the weights.

The tilde notation means that we have included the offset term $$w_0$$, also known as the **bias**:

$$
\tilde{\pmb{x}}_n=\begin{bmatrix}1 \\ x_{n1} \\ \vdots \\ x_{nD} \end{bmatrix} \in \mathbb{R}^{D+1}, 
\quad
\tilde{\pmb{w}} = \begin{bmatrix}w_0 \\ w_1 \\ \vdots \\ w_D\end{bmatrix} \in \mathbb{R^{D+1}}
$$

### The $$D > N$$ problem

If the number of parameters exceeds the number of data examples, we say that the task is *under-determined*. This can be solved by regularization, which weâ€™ll get to more precisely later



## Cost functions

$$\pmb{x}_n$$ is the data, which we can easily understand where comes from. But how does one find a good $$\pmb{w}$$ from the data? 

A **cost function** (also called loss function) is used to learn parameters that explain the data well. It quantifies how well our model does by giving errors a score, quantifying penalties for errors. Our goal is to find weights that minimize the loss functions.

### Properties

Desirable properties of cost functions are:

- **Symmetry around 0**: that is, being off by a positive or negative amount is equivalent; what matters is the amplitude of the error, not the sign.
- **Robustness**: penalizes large errors at about the same rate as very large errors. This is a way to make sure that outliers donâ€™t completely dominate our regression.

### Good cost functions

Probably the most commonly used cost function is Mean Square Error (MSE): 

$$
\text{MSE}(\pmb{w}) := \frac{1}{N} \sum_{n=1}^N \left(y_n - f(\pmb{x}_n)\right)^2
$$

MSE is symmetrical aroudn 0, but also tends to penalize outliers quite harshly (because it squares error): MSE is not robust. In practice, this is problematic, because outliers occur more often than weâ€™d like to.

When outliers are present, Mean Absolute Error (MAE) tends to fare better:

$$
\text{MAE}(\pmb{w}) := \frac{1}{N} \sum_{n=1}^N \left| y_n - f(\pmb{x}_n)\right|
$$

Instead of squaring, we take the absolute value. This is more robust. Note that MAE isnâ€™t differentiable at 0, but weâ€™ll talk about that later.

There are other cost functions that are even more robust; these are available as additional reading, but is not exam material.

### Convexity

A function is **convex** iff a line joining two points never intersects with the function anywhere else. More strictly defined, a function $$f(\pmb{u})$$ with $$\pmb{u}\in\chi$$ is *convex* if, for any $$\pmb{u}, \pmb{v} \in\chi$$, and for any $$0 \le\lambda\le 1$$, we have:

$$
f(\lambda\pmb{u}+(1-\lambda)\pmb{v})\le\lambda f(\pmb{u}) +(1-\lambda)f(\pmb{v})
$$

A function is **strictly convex** if the above inequality is strict ($$<$$).

A stritly convex function has a unique global minimum $$\pmb{w}^*$$. For convex functions, every local minimum is a global minimum. This makes it a desirable property for loss functions, since it means that cost function optimization is guaranteed to find the global minimum.

Sums of convex functions are also convex. Therefore, MSE and MAE are convex.